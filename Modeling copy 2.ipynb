{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import nbformat\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n",
    "\n",
    "#### Loading Data\n",
    "product_profile = pd.read_csv(\"product_profile.csv\")\n",
    "user_profile = pd.read_csv(\"user_profile.csv\")\n",
    "order = pd.read_csv(\"orders_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "      <th>vendor</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_length</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Gizmo</td>\n",
       "      <td>29.4633</td>\n",
       "      <td>Rustic Paper Wallet</td>\n",
       "      <td>Swaniawski, Casper and Hilll</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>172.750000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Doohickey</td>\n",
       "      <td>70.0799</td>\n",
       "      <td>Small Marble Shoes</td>\n",
       "      <td>Balistreri-Ankunding</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Doohickey</td>\n",
       "      <td>35.3887</td>\n",
       "      <td>Synergistic Granite Chair</td>\n",
       "      <td>Murray, Watsica and Wunsch</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Doohickey</td>\n",
       "      <td>73.9918</td>\n",
       "      <td>Enormous Aluminum Shirt</td>\n",
       "      <td>Regan Bradtke and Sons</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>167.600000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gadget</td>\n",
       "      <td>82.7451</td>\n",
       "      <td>Enormous Marble Wallet</td>\n",
       "      <td>Price, Schultz and Daniel</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>146.750000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>Widget</td>\n",
       "      <td>46.7641</td>\n",
       "      <td>Heavy-Duty Linen Toucan</td>\n",
       "      <td>Balistreri-Muller</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>Gizmo</td>\n",
       "      <td>46.7641</td>\n",
       "      <td>Aerodynamic Concrete Lamp</td>\n",
       "      <td>Erika Volkman Group</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>162.833333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>Gizmo</td>\n",
       "      <td>46.7641</td>\n",
       "      <td>Enormous Copper Shirt</td>\n",
       "      <td>Considine, Schamberger and Schiller</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>Widget</td>\n",
       "      <td>76.9533</td>\n",
       "      <td>Mediocre Leather Coat</td>\n",
       "      <td>Gulgowski, Grimes and Mayer</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>185.333333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>Gadget</td>\n",
       "      <td>48.8026</td>\n",
       "      <td>Intelligent Steel Car</td>\n",
       "      <td>Pouros, Nitzsche and Mayer</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   category    price                      title  \\\n",
       "0      1      Gizmo  29.4633        Rustic Paper Wallet   \n",
       "1      2  Doohickey  70.0799         Small Marble Shoes   \n",
       "2      3  Doohickey  35.3887  Synergistic Granite Chair   \n",
       "3      4  Doohickey  73.9918    Enormous Aluminum Shirt   \n",
       "4      5     Gadget  82.7451     Enormous Marble Wallet   \n",
       "..   ...        ...      ...                        ...   \n",
       "195  196     Widget  46.7641    Heavy-Duty Linen Toucan   \n",
       "196  197      Gizmo  46.7641  Aerodynamic Concrete Lamp   \n",
       "197  198      Gizmo  46.7641      Enormous Copper Shirt   \n",
       "198  199     Widget  76.9533      Mediocre Leather Coat   \n",
       "199  200     Gadget  48.8026      Intelligent Steel Car   \n",
       "\n",
       "                                  vendor  average_rating  review_length  \\\n",
       "0           Swaniawski, Casper and Hilll        4.625000     172.750000   \n",
       "1                   Balistreri-Ankunding        0.000000       0.000000   \n",
       "2             Murray, Watsica and Wunsch        4.000000     171.000000   \n",
       "3                 Regan Bradtke and Sons        3.000000     167.600000   \n",
       "4              Price, Schultz and Daniel        4.000000     146.750000   \n",
       "..                                   ...             ...            ...   \n",
       "195                    Balistreri-Muller        0.000000       0.000000   \n",
       "196                  Erika Volkman Group        4.666667     162.833333   \n",
       "197  Considine, Schamberger and Schiller        4.142857     197.000000   \n",
       "198          Gulgowski, Grimes and Mayer        3.666667     185.333333   \n",
       "199           Pouros, Nitzsche and Mayer        4.000000     204.000000   \n",
       "\n",
       "     review_count  \n",
       "0             8.0  \n",
       "1             0.0  \n",
       "2             7.0  \n",
       "3             5.0  \n",
       "4             4.0  \n",
       "..            ...  \n",
       "195           0.0  \n",
       "196           6.0  \n",
       "197           7.0  \n",
       "198           6.0  \n",
       "199           6.0  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_profile = product_profile.fillna(0)\n",
    "product_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>source</th>\n",
       "      <th>dayduration</th>\n",
       "      <th>age</th>\n",
       "      <th>total_spent_Doohickey</th>\n",
       "      <th>total_spent_Gadget</th>\n",
       "      <th>total_spent_Gizmo</th>\n",
       "      <th>total_spent_Widget</th>\n",
       "      <th>total_orders</th>\n",
       "      <th>discount_usage_proportion</th>\n",
       "      <th>email_provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hudson Borer</td>\n",
       "      <td>NE</td>\n",
       "      <td>40.7132</td>\n",
       "      <td>-98.5260</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2684</td>\n",
       "      <td>38</td>\n",
       "      <td>189.5193</td>\n",
       "      <td>389.5355</td>\n",
       "      <td>221.8629</td>\n",
       "      <td>1719.2326</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Domenica Williamson</td>\n",
       "      <td>IA</td>\n",
       "      <td>41.5813</td>\n",
       "      <td>-92.6991</td>\n",
       "      <td>Affiliate</td>\n",
       "      <td>2500</td>\n",
       "      <td>57</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Lina Heaney</td>\n",
       "      <td>MN</td>\n",
       "      <td>46.1197</td>\n",
       "      <td>-92.8416</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>2786</td>\n",
       "      <td>63</td>\n",
       "      <td>896.4755</td>\n",
       "      <td>126.9100</td>\n",
       "      <td>695.0698</td>\n",
       "      <td>510.8554</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Arnold Adams</td>\n",
       "      <td>CO</td>\n",
       "      <td>37.9203</td>\n",
       "      <td>-104.9730</td>\n",
       "      <td>Google</td>\n",
       "      <td>2182</td>\n",
       "      <td>32</td>\n",
       "      <td>149.8910</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>214.7897</td>\n",
       "      <td>150.5928</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>gmail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dominique Leffler</td>\n",
       "      <td>NY</td>\n",
       "      <td>42.3490</td>\n",
       "      <td>-77.0567</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2716</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>332.2080</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>hotmail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 name state  latitude  longitude     source  dayduration  \\\n",
       "0   1         Hudson Borer    NE   40.7132   -98.5260    Twitter         2684   \n",
       "1   2  Domenica Williamson    IA   41.5813   -92.6991  Affiliate         2500   \n",
       "2   3          Lina Heaney    MN   46.1197   -92.8416   Facebook         2786   \n",
       "3   4         Arnold Adams    CO   37.9203  -104.9730     Google         2182   \n",
       "4   5    Dominique Leffler    NY   42.3490   -77.0567    Twitter         2716   \n",
       "\n",
       "   age  total_spent_Doohickey  total_spent_Gadget  total_spent_Gizmo  \\\n",
       "0   38               189.5193            389.5355           221.8629   \n",
       "1   57                 0.0000              0.0000             0.0000   \n",
       "2   63               896.4755            126.9100           695.0698   \n",
       "3   32               149.8910              0.0000           214.7897   \n",
       "4   50                 0.0000              0.0000           332.2080   \n",
       "\n",
       "   total_spent_Widget  total_orders  discount_usage_proportion email_provider  \n",
       "0           1719.2326          11.0                   0.272727          yahoo  \n",
       "1              0.0000           0.0                   0.000000          yahoo  \n",
       "2            510.8554          10.0                   0.200000          yahoo  \n",
       "3            150.5928           4.0                   0.250000          gmail  \n",
       "4              0.0000           1.0                   0.000000        hotmail  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profile = user_profile.drop(\"Unnamed: 0\", axis= 1)\n",
    "user_profile = user_profile.fillna(0)\n",
    "user_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35756</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1861</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35757</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1861</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35758</th>\n",
       "      <td>NaN</td>\n",
       "      <td>554</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35759</th>\n",
       "      <td>NaN</td>\n",
       "      <td>554</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35760</th>\n",
       "      <td>NaN</td>\n",
       "      <td>554</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  user_id  product_id\n",
       "35756         NaN     1861         136\n",
       "35757         NaN     1861         136\n",
       "35758         NaN      554         197\n",
       "35759         NaN      554         197\n",
       "35760         NaN      554         197"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = order[[\"user_id\", \"product_id\"]]\n",
    "interactions = interactions.sort_index()\n",
    "interactions = pd.read_csv(\"interaction.csv\")\n",
    "interactions.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Modeling Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Unnecessary columns and records\n",
    "user_profile = user_profile.drop([\"longitude\",\"state\",\"name\"], axis= 1)\n",
    "user_profile = user_profile.loc[user_profile['total_orders'] != 0]\n",
    "product_profile = product_profile.drop([\"title\",\"vendor\"],axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile = pd.get_dummies(user_profile, columns=['source', 'email_provider'])\n",
    "product_profile = pd.get_dummies(product_profile, columns=['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numeric_cols_user = ['latitude', 'dayduration', 'age', 'total_spent_Doohickey',\n",
    "                     'total_spent_Gadget', 'total_spent_Gizmo', 'total_spent_Widget', 'total_orders', 'discount_usage_proportion']\n",
    "user_profile[numeric_cols_user] = scaler.fit_transform(user_profile[numeric_cols_user])\n",
    "\n",
    "numeric_cols_product = ['price', 'average_rating', 'review_length', 'review_count']\n",
    "product_profile[numeric_cols_product] = scaler.fit_transform(product_profile[numeric_cols_product])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'price', 'average_rating', 'review_length', 'review_count',\n",
       "       'category_Doohickey', 'category_Gadget', 'category_Gizmo',\n",
       "       'category_Widget'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_profile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'latitude', 'dayduration', 'age', 'total_spent_Doohickey',\n",
       "       'total_spent_Gadget', 'total_spent_Gizmo', 'total_spent_Widget',\n",
       "       'total_orders', 'discount_usage_proportion', 'source_Affiliate',\n",
       "       'source_Facebook', 'source_Google', 'source_Organic', 'source_Twitter',\n",
       "       'email_provider_gmail', 'email_provider_hotmail',\n",
       "       'email_provider_yahoo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profile.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Collaborative Filtering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.0211\n",
      "Epoch 2, Loss: 4.1048\n",
      "Epoch 3, Loss: 4.0195\n",
      "Epoch 4, Loss: 4.0750\n",
      "Epoch 5, Loss: 4.0829\n",
      "Epoch 6, Loss: 4.1209\n",
      "Epoch 7, Loss: 4.0896\n",
      "Epoch 8, Loss: 4.0796\n",
      "Epoch 9, Loss: 4.0721\n",
      "Epoch 10, Loss: 4.0801\n",
      "Epoch 11, Loss: 4.1072\n",
      "Epoch 12, Loss: 4.1324\n",
      "Epoch 13, Loss: 4.0946\n",
      "Epoch 14, Loss: 4.0708\n",
      "Epoch 15, Loss: 4.0855\n",
      "Epoch 16, Loss: 4.0767\n",
      "Epoch 17, Loss: 4.0504\n",
      "Epoch 18, Loss: 4.0175\n",
      "Epoch 19, Loss: 4.0404\n",
      "Epoch 20, Loss: 4.0291\n",
      "Epoch 21, Loss: 4.0615\n",
      "Epoch 22, Loss: 4.0615\n",
      "Epoch 23, Loss: 4.0624\n",
      "Epoch 24, Loss: 4.0452\n",
      "Epoch 25, Loss: 4.0564\n",
      "Epoch 26, Loss: 4.0822\n",
      "Epoch 27, Loss: 4.0557\n",
      "Epoch 28, Loss: 4.0451\n",
      "Epoch 29, Loss: 4.0358\n",
      "Epoch 30, Loss: 4.0368\n",
      "Epoch 31, Loss: 4.0394\n",
      "Epoch 32, Loss: 4.0412\n",
      "Epoch 33, Loss: 4.0469\n",
      "Epoch 34, Loss: 4.0337\n",
      "Epoch 35, Loss: 4.0283\n",
      "Epoch 36, Loss: 4.0432\n",
      "Epoch 37, Loss: 4.0405\n",
      "Epoch 38, Loss: 4.0325\n",
      "Epoch 39, Loss: 4.0541\n",
      "Epoch 40, Loss: 4.0372\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==========================\n",
    "# 1. Data Preparation\n",
    "# ==========================\n",
    "\n",
    "# For reproducibility:\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---- Split interactions into train and test (backtesting) ----\n",
    "# For each user, the last interaction (by the ordering of interactions) is held out as test.\n",
    "# Count the number of interactions for each user\n",
    "user_interaction_count = interactions.groupby('user_id').size()\n",
    "\n",
    "# Filter users who have more than 1 interaction\n",
    "valid_users = user_interaction_count[user_interaction_count > 1].index\n",
    "\n",
    "# Select only the interactions of valid users\n",
    "valid_interactions = interactions[interactions['user_id'].isin(valid_users)]\n",
    "\n",
    "# Get the last interaction for each user\n",
    "test_interactions = valid_interactions.groupby('user_id').tail(2)\n",
    "\n",
    "# Get the rest as the training set\n",
    "train_interactions = valid_interactions.drop(test_interactions.index)\n",
    "\n",
    "# ---- Create mapping dictionaries for users and products ----\n",
    "user_ids = user_profile['id'].unique()\n",
    "product_ids = product_profile['id'].unique()\n",
    "user_map = {uid: i for i, uid in enumerate(user_ids)}\n",
    "product_map = {pid: i for i, pid in enumerate(product_ids)}\n",
    "\n",
    "# ---- Prepare features for users and products ----\n",
    "# Choose which columns to use.\n",
    "# (For the categorical features you might want to apply encoding; here we assume they are already numeric.)\n",
    "user_feature_cols = ['latitude', 'dayduration', 'age', 'total_spent_Doohickey',\n",
    "       'total_spent_Gadget', 'total_spent_Gizmo', 'total_spent_Widget',\n",
    "       'total_orders', 'discount_usage_proportion', 'source_Affiliate',\n",
    "       'source_Facebook', 'source_Google', 'source_Organic', 'source_Twitter',\n",
    "       'email_provider_gmail', 'email_provider_hotmail',\n",
    "       'email_provider_yahoo']\n",
    "\n",
    "product_feature_cols = ['price', 'average_rating', 'review_length', 'review_count',\n",
    "       'category_Doohickey', 'category_Gadget', 'category_Gizmo',\n",
    "       'category_Widget']\n",
    "\n",
    "# Set indices for fast lookup\n",
    "user_profile_indexed = user_profile.set_index('id')\n",
    "product_profile_indexed = product_profile.set_index('id')\n",
    "\n",
    "def get_user_features(uid):\n",
    "    \"\"\"Return the feature vector for a given user id as a NumPy array.\"\"\"\n",
    "    return user_profile_indexed.loc[uid, user_feature_cols].values.astype(np.float32)\n",
    "\n",
    "def get_product_features(pid):\n",
    "    \"\"\"Return the feature vector for a given product id as a NumPy array.\"\"\"\n",
    "    return product_profile_indexed.loc[pid, product_feature_cols].values.astype(np.float32)\n",
    "\n",
    "# Build a product features matrix for all products (ordered by product_map)\n",
    "num_products = len(product_map)\n",
    "product_features_matrix = np.zeros((num_products, len(product_feature_cols)), dtype=np.float32)\n",
    "for pid, idx in product_map.items():\n",
    "    product_features_matrix[idx] = get_product_features(pid)\n",
    "# Convert to a PyTorch tensor (this tensor is fixed and will be used to generate product embeddings)\n",
    "product_features_tensor = torch.tensor(product_features_matrix)\n",
    "\n",
    "# ==========================\n",
    "# 2. Create PyTorch Dataset\n",
    "# ==========================\n",
    "\n",
    "class RecommenderDataset(Dataset):\n",
    "    def __init__(self, interactions_df):\n",
    "        # Reset index so that we can iterate row-by-row\n",
    "        self.interactions_df = interactions_df.reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.interactions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.interactions_df.iloc[idx]\n",
    "        uid = row['user_id']\n",
    "        pid = row['product_id']\n",
    "        user_feat = get_user_features(uid)\n",
    "        # The label is the product index (for multiclass classification)\n",
    "        label = product_map[pid]\n",
    "        return torch.tensor(user_feat), torch.tensor(label).long()\n",
    "        \n",
    "# Create training and testing datasets and loaders\n",
    "train_dataset = RecommenderDataset(train_interactions)\n",
    "test_dataset = RecommenderDataset(test_interactions)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ==========================\n",
    "# 3. Define the Two-Tower Model\n",
    "# ==========================\n",
    "\n",
    "class RecommenderModel(nn.Module):\n",
    "    def __init__(self, user_input_dim, product_input_dim, embedding_dim, product_features_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_input_dim: Dimension of the user feature vector.\n",
    "            product_input_dim: Dimension of the product feature vector.\n",
    "            embedding_dim: Size of the common embedding space.\n",
    "            product_features_tensor: A tensor of shape (num_products, product_input_dim)\n",
    "                                     containing the features for each product.\n",
    "        \"\"\"\n",
    "        super(RecommenderModel, self).__init__()\n",
    "        # --- User Tower ---\n",
    "        self.user_net = nn.Sequential(\n",
    "            nn.Linear(user_input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)\n",
    "        )\n",
    "        # --- Product Tower ---\n",
    "        self.product_net = nn.Sequential(\n",
    "            nn.Linear(product_input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)\n",
    "        )\n",
    "        # Register the product features tensor as a buffer so it is part of the graph\n",
    "        self.register_buffer('product_features', product_features_tensor)\n",
    "        \n",
    "    def forward(self, user_features):\n",
    "        \"\"\"\n",
    "        Given a batch of user features (shape: [batch_size, user_input_dim]),\n",
    "        compute the user embedding and then score all products via dot-product.\n",
    "        Returns:\n",
    "            logits: Tensor of shape [batch_size, num_products] (raw scores).\n",
    "        \"\"\"\n",
    "        # Compute user embedding for the batch.\n",
    "        user_emb = self.user_net(user_features)  # shape: (batch, embedding_dim)\n",
    "        # Compute product embeddings for all products (using the fixed product features).\n",
    "        product_emb = self.product_net(self.product_features)  # shape: (num_products, embedding_dim)\n",
    "        # Compute dot-product between each user and all products.\n",
    "        logits = torch.matmul(user_emb, product_emb.t())  # shape: (batch, num_products)\n",
    "        # (Optionally, you could apply softmax here to get probabilities, but for training with CrossEntropyLoss itâ€™s not needed.)\n",
    "        return logits\n",
    "\n",
    "# Set hyperparameters\n",
    "user_input_dim = len(user_feature_cols)\n",
    "product_input_dim = len(product_feature_cols)\n",
    "embedding_dim = 32  # You can tune this\n",
    "\n",
    "model = RecommenderModel(user_input_dim, product_input_dim, embedding_dim, product_features_tensor)\n",
    "\n",
    "# ==========================\n",
    "# 4. Training Setup\n",
    "# ==========================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # This loss expects raw logits.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_user, batch_label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_user)  # shape: (batch, num_products)\n",
    "            loss = criterion(logits, batch_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate @ 5: 0.5382\n"
     ]
    }
   ],
   "source": [
    "def hit_rate_at_k(model, test_loader, k=5):\n",
    "    \"\"\"\n",
    "    Computes the Hit Rate@K metric over the test set.\n",
    "    For each test example, if the true product label is among the top-K predicted products, \n",
    "    it counts as a hit.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_user, batch_label in test_loader:\n",
    "            logits = model(batch_user)  # (batch, num_products)\n",
    "            # (Optional) If you need probabilities, you can do:\n",
    "            # probs = torch.softmax(logits, dim=1)\n",
    "            # Get indices of the top K predictions.\n",
    "            topk = torch.topk(logits, k, dim=1).indices  # shape: (batch, k)\n",
    "            for i in range(len(batch_label)):\n",
    "                total += 1\n",
    "                if batch_label[i] in topk[i]:\n",
    "                    hits += 1\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "hit_rate = hit_rate_at_k(model, test_loader, k=100)\n",
    "print(f\"Hit Rate @ 5: {hit_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate @ 5 (Training Set): 0.7589\n"
     ]
    }
   ],
   "source": [
    "def hit_rate_at_k_train(model, train_loader, k=5):\n",
    "    \"\"\"\n",
    "    Computes the Hit Rate@K metric over the training set.\n",
    "    For each train example, if the true product label is among the top-K predicted products, \n",
    "    it counts as a hit.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_user, batch_label in train_loader:\n",
    "            logits = model(batch_user)  # shape: (batch, num_products)\n",
    "            # (Optional) If you need probabilities, you can do:\n",
    "            # probs = torch.softmax(logits, dim=1)\n",
    "            # Get indices of the top K predictions.\n",
    "            topk = torch.topk(logits, k, dim=1).indices  # shape: (batch, k)\n",
    "            for i in range(len(batch_label)):\n",
    "                total += 1\n",
    "                if batch_label[i] in topk[i]:\n",
    "                    hits += 1\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "# Compute Hit Rate @ 5 for the training set\n",
    "hit_rate_train = hit_rate_at_k_train(model, train_loader, k=100)\n",
    "print(f\"Hit Rate @ 5 (Training Set): {hit_rate_train:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
